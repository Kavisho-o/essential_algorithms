{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a30cf5",
   "metadata": {},
   "source": [
    "## California Housing\n",
    "\n",
    "~ Using baseline LR\n",
    "\n",
    "~ Checking if scaling helps (in theory it shouldnt)\n",
    "\n",
    "~ Checking if using polynomial feature helps \n",
    "\n",
    "~ Add irrelevant features (noise) \n",
    "\n",
    "~ Check multicollinearity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beb0e5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51e98b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7f20fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_california_housing(as_frame=True)\n",
    "df = data.frame\n",
    "\n",
    "X = df.drop(\"MedHouseVal\", axis=1)\n",
    "y = df[\"MedHouseVal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb5aaedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa4bd0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2: 0.6125511913966952\n",
      "Test R2: 0.575787706032451\n",
      "Train MSE: 0.5179331255246699\n",
      "Test MSE: 0.5558915986952442\n"
     ]
    }
   ],
   "source": [
    "# Baseline Linear Regression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "train_preds = model.predict(X_train)\n",
    "test_preds = model.predict(X_test)\n",
    "\n",
    "train_r2 = r2_score(y_train, train_preds)\n",
    "test_r2 = r2_score(y_test, test_preds)\n",
    "\n",
    "train_mse = mean_squared_error(y_train, train_preds)\n",
    "test_mse = mean_squared_error(y_test, test_preds)\n",
    "\n",
    "print(\"Train R2:\", train_r2)\n",
    "print(\"Test R2:\", test_r2)\n",
    "print(\"Train MSE:\", train_mse)\n",
    "print(\"Test MSE:\", test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cac4127a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2 (scaled): 0.6125511913966952\n",
      "Test R2 (scaled): 0.5757877060324508\n"
     ]
    }
   ],
   "source": [
    "# LR with scaling \n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model_scaled = LinearRegression()\n",
    "model_scaled.fit(X_train_scaled, y_train)\n",
    "\n",
    "train_preds_scaled = model_scaled.predict(X_train_scaled)\n",
    "test_preds_scaled = model_scaled.predict(X_test_scaled)\n",
    "\n",
    "print(\"Train R2 (scaled):\", r2_score(y_train, train_preds_scaled))\n",
    "print(\"Test R2 (scaled):\", r2_score(y_test, test_preds_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402db081",
   "metadata": {},
   "source": [
    "Observation: as expected, scaling had no effect on r2 score for lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31326965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2 (poly): 0.6852681982344955\n",
      "Test R2 (poly): 0.6456819728455382\n"
     ]
    }
   ],
   "source": [
    "## Using polynomial features\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "model_poly = LinearRegression()\n",
    "model_poly.fit(X_train_poly, y_train)\n",
    "\n",
    "train_preds_poly = model_poly.predict(X_train_poly)\n",
    "test_preds_poly = model_poly.predict(X_test_poly)\n",
    "\n",
    "print(\"Train R2 (poly):\", r2_score(y_train, train_preds_poly))\n",
    "print(\"Test R2 (poly):\", r2_score(y_test, test_preds_poly))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0a128f",
   "metadata": {},
   "source": [
    "Observation:\n",
    " \n",
    "train r2 increases, test r2 also increased slightly\n",
    "\n",
    "GAP between them increased\n",
    "\n",
    "as expected tho, theory said that increasing model complexity will lead to lower bias and higher variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6234d2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R2 (with noise): 0.5757174560575071\n"
     ]
    }
   ],
   "source": [
    "## adding irrelevant features\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X_train_noise = X_train.copy()\n",
    "X_test_noise = X_test.copy()\n",
    "\n",
    "X_train_noise[\"random_noise\"] = np.random.randn(len(X_train))\n",
    "X_test_noise[\"random_noise\"] = np.random.randn(len(X_test))\n",
    "\n",
    "model_noise = LinearRegression()\n",
    "model_noise.fit(X_train_noise, y_train)\n",
    "\n",
    "test_preds_noise = model_noise.predict(X_test_noise)\n",
    "\n",
    "print(\"Test R2 (with noise):\", r2_score(y_test, test_preds_noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b0423af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        feature  coefficient\n",
      "3     AveBedrms     0.783328\n",
      "0        MedInc     0.448615\n",
      "7     Longitude    -0.433671\n",
      "6      Latitude    -0.419773\n",
      "2      AveRooms    -0.123351\n",
      "1      HouseAge     0.009727\n",
      "8  random_noise     0.007913\n",
      "5      AveOccup    -0.003517\n",
      "4    Population    -0.000002\n"
     ]
    }
   ],
   "source": [
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": X_train_noise.columns,\n",
    "    \"coefficient\": model_noise.coef_\n",
    "})\n",
    "\n",
    "print(coef_df.sort_values(by=\"coefficient\", key=abs, ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d6120e",
   "metadata": {},
   "source": [
    "Observation:\n",
    "\n",
    "coeff of noise is non zero, so model tried to use it\n",
    "\n",
    "this added variance which caused r2 score to drop slightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bb4997a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R2 (multicollinearity): 0.5757877060324506\n"
     ]
    }
   ],
   "source": [
    "## testing multicollinearity (duplicating a feature)\n",
    "\n",
    "X_train_multi = X_train.copy()\n",
    "X_test_multi = X_test.copy()\n",
    "\n",
    "X_train_multi[\"duplicate_income\"] = X_train_multi[\"MedInc\"]\n",
    "X_test_multi[\"duplicate_income\"] = X_test_multi[\"MedInc\"]\n",
    "\n",
    "model_multi = LinearRegression()\n",
    "model_multi.fit(X_train_multi, y_train)\n",
    "\n",
    "test_preds_multi = model_multi.predict(X_test_multi)\n",
    "\n",
    "print(\"Test R2 (multicollinearity):\", r2_score(y_test, test_preds_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44ad7369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            feature  coefficient\n",
      "3         AveBedrms     0.783145\n",
      "7         Longitude    -0.433708\n",
      "6          Latitude    -0.419792\n",
      "0            MedInc     0.224337\n",
      "8  duplicate_income     0.224337\n",
      "2          AveRooms    -0.123323\n",
      "1          HouseAge     0.009724\n",
      "5          AveOccup    -0.003526\n",
      "4        Population    -0.000002\n"
     ]
    }
   ],
   "source": [
    "coef_multi = pd.DataFrame({\n",
    "    \"feature\": X_train_multi.columns,\n",
    "    \"coefficient\": model_multi.coef_\n",
    "})\n",
    "\n",
    "print(coef_multi.sort_values(by=\"coefficient\", key=abs, ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022bcd5b",
   "metadata": {},
   "source": [
    "Observation: \n",
    "\n",
    "coeff of medinc and duplicate_income are unstable now\n",
    "\n",
    "infinitely many solutions giving same predictions when features are perfectly correlated\n",
    "\n",
    "r2 same cause multicollinearity increases variance of coefficient estimates but not necessarily prediction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26a4257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why did polynomial increase variance?\n",
    "\n",
    "\n",
    "# Why didn’t scaling change performance?\n",
    "\n",
    "# Why were predictions stable but coefficients unstable?\n",
    "\n",
    "# What would Ridge regression fix here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f68cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions:\n",
    "\n",
    "# Why did polynomial increase variance?\n",
    "# Because it added more features, which can lead to overfitting, \n",
    "# especially if the new features are not informative or if the model is too complex for the amount of data available.\n",
    "\n",
    "# Why didn’t scaling change performance?\n",
    "# Because linear regression is not sensitive to the scale of the features,\n",
    "# as it can find the optimal coefficients regardless of the feature scales.\n",
    "\n",
    "# Why were predictions stable but coefficients unstable?\n",
    "# Because of multicollinearity, where two or more features are highly correlated,\n",
    "# leading to large changes in coefficients with small changes in the data, while predictions remain stable.\n",
    "\n",
    "# What would Ridge regression fix here?\n",
    "# Ridge regression adds L2 regularization, which can help mitigate multicollinearity by shrinking the coefficients,\n",
    "# leading to more stable coefficients and potentially better generalization on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513efbba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
